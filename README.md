# Lumia - Lokal AI-chattj√§nst med personligt minne

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com)
[![Ollama](https://img.shields.io/badge/Ollama-Local-orange.svg)](https://ollama.ai)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-VectorDB-purple.svg)](https://chromadb.com)

> **Lumia** √§r en lokal, s√§ker och personlig AI-chattj√§nst som kombinerar styrkan i stora spr√•kmodeller (LLM) med ett kontinuerligt v√§xande personligt minne. Genom att koppla ihop en lokal spr√•kmodell (via Ollama) med en s√∂kmotor f√∂r kontext (Brain-tj√§nsten) som anv√§nder RAG och ChromaDB, bygger Lumia upp en anv√§ndarspecifik f√∂rst√•else som f√∂rb√§ttras √∂ver tid.

## üéØ Vision

M√•let √§r att skapa en privat, responsiv assistent som f√∂rst√•r vem du √§r och anpassar sina svar ‚Äì utan att skicka n√•gon data till molnet.

## üöÄ Huvudfunktioner

- **Chatbaserad anv√§ndarupplevelse** med direkt streaming av svar fr√•n lokal spr√•kmodell
- **Anv√§ndarspecifikt minne** som v√§xer √∂ver tid genom kontinuerlig ingestion av dialoghistorik
- **Parallell RAG-analys** via mikrotj√§nsten *Brain* f√∂r kontextuell f√∂rst√§rkning
- **S√§ker och lokal datalagring** via ChromaDB, med en collection per anv√§ndare
- **Registrering och inloggning** (med planerad JWT-autentisering)
- **Framtidss√§krad struktur** f√∂r att ut√∂ka med olika typer av kontextstrategier och specialiserade agents

## üèóÔ∏è Systemarkitektur

```mermaid
graph TB
    A[Frontend - Lumia Chat UI] --> B[Python Backend - FastAPI]
    B --> C[Ollama LLM - Gemma 3:12b]
    B --> D[Brain API - FastAPI]
    D --> E[ChromaDB - Kundspecifika collections]
    D --> F[Ollama - Embeddings: nomic-embed-text]
    B --> G[Anv√§ndardatabas - SQLite/PostgreSQL]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#fff3e0
    style D fill:#e8f5e8
    style E fill:#fce4ec
    style F fill:#fff3e0
    style G fill:#f1f8e9
```

## üìã Komponenter

### üí¨ LLM Chatmotor (via Ollama)
- Lokal LLM-modell, t.ex. `qwen2.5:7b`
- Streamar svaret f√∂r snabb interaktion
- Anv√§nds alltid som prim√§r generator

### üß† Brain RAG Service
- Frist√•ende FastAPI-mikrotj√§nst
- Tar emot fr√•gor och s√∂ker i kundspecifika ChromaDB-collections
- RAG-modell bygger svar via Ollama
- All embedding-generering sker lokalt (nomic-embed-text)
- Dashboard f√∂r dokumenthantering

### üìö Kontextinjektion
- Svar fr√•n *Brain* kan komplettera eller ers√§tta svar fr√•n LLM
- Regler beh√∂vs f√∂r n√§r RAG-kontext ska anv√§ndas

### üë§ Anv√§ndarhantering
- Registrering, inloggning och autentisering via JWT
- Varje anv√§ndare tilldelas en unik collection i Brain, ex: `lumia_100023`
- Alla chattar kan sparas automatiskt i denna collection
- M√∂jlighet att visa historik i framtiden via dashboard eller export

## üõ†Ô∏è Teknisk Stack

| Komponent | Verktyg / Modell | Notering |
|-----------|------------------|----------|
| **LLM** | Ollama (`gemma3:12b`) | Lokal, 12B parametrar |
| **Embeddings** | Ollama (`nomic-embed-text`) | Lokal, vektorer |
| **Vektordatabas** | ChromaDB | Per anv√§ndare |
| **Backend** | Python, FastAPI | JWT, routing |
| **Kontextmotor** | Brain (FastAPI) | RAG med LLM |
| **Orkestrering** | LangChain | Framtida kedjor |
| **Anv√§ndardata** | PostgreSQL / SQLite | Inloggning |
| **UI** | Web/terminal/chatbot | Anpassningsbar |

## üì¶ Installation

### F√∂ruts√§ttningar

1. **Ollama** installerat och k√∂rande
2. **Python 3.8+**
3. **Docker** (valfritt f√∂r ChromaDB)

### Snabbstart

```bash
# Klona projektet
git clone <repository-url>
cd lumia

# Installera dependencies
pip install -r requirements.txt

# Starta Brain-tj√§nsten
cd brain
uvicorn main:app --reload --port 8001

# Starta huvudapplikationen (i ny terminal)
cd ..
uvicorn main:app --reload --port 8000
```

### Ollama-modeller

```bash
# Ladda ner LLM-modell
ollama pull gemma3:12b

# Ladda ner embedding-modell
ollama pull nomic-embed-text
```

## üîß Konfiguration

Skapa en `.env`-fil i projektroten:

```env
# Ollama-inst√§llningar
OLLAMA_BASE_URL=http://localhost:11434
LLM_MODEL=qwen2.5:7b
EMBEDDING_MODEL=nomic-embed-text

# Brain API
BRAIN_API_URL=http://localhost:8001

# Databas
DATABASE_URL=sqlite:///./lumia.db

# JWT
SECRET_KEY=your-secret-key-here
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# ChromaDB
CHROMA_PERSIST_DIRECTORY=./chroma_db
```

## üöÄ Anv√§ndning

### API-anrop mot Brain

**/ingest**
Spara ny konversation:

```json
POST /ingest
{
  "customer_id": "lumia_100023",
  "content": "Anv√§ndarens fr√•ga + LLM-svar",
  "metadata": {
    "source": "chat",
    "timestamp": "2025-08-03T12:00:00Z"
  }
}
```

**/query**
S√∂k i tidigare kontext:

```json
POST /query
{
  "customer_id": "lumia_100023",
  "question": "Vad sa jag f√∂rra veckan om API-nycklar?",
  "n_results": 3
}
```

### Exempel p√• anv√§ndning

```python
import requests

# Skicka chattmeddelande
response = requests.post("http://localhost:8000/chat", json={
    "message": "Vad sa jag f√∂rra veckan om API-nycklar?",
    "user_id": "lumia_100023"
})

# Svaret streamas tillbaka
for chunk in response.iter_content(chunk_size=1024):
    print(chunk.decode(), end='')
```

## üìà Roadmap

| Version | Funktioner                                               |
| ------- | -------------------------------------------------------- |
| v0.1    | Chatgr√§nssnitt + streaming + enkel Brain-koppling        |
| v0.2    | Inloggning + JWT + sparande till ChromaDB                |
| v0.3    | Kontextstrategier (rules for retrieval)                  |
| v0.4    | Historikvy + anv√§ndarprofil                              |
| v0.5    | Multi-agent support + integrationer (e-post, Slack, etc) |

## ü§ù Bidrag

1. Forka projektet
2. Skapa en feature branch (`git checkout -b feature/amazing-feature`)
3. Committa dina √§ndringar (`git commit -m 'Add amazing feature'`)
4. Pusha till branchen (`git push origin feature/amazing-feature`)
5. √ñppna en Pull Request

## üìÑ Licens

Detta projekt √§r licensierat under MIT-licensen - se [LICENSE](LICENSE) filen f√∂r detaljer.

## üÜò Support

Om du st√∂ter p√• problem eller har fr√•gor:

1. Kontrollera [Issues](https://github.com/your-repo/lumia/issues)
2. Skapa en ny issue med detaljerad beskrivning
3. Kontakta utvecklingsteamet

---

**Lumia** - Din lokala AI-assistent med personligt minne üß†‚ú® 