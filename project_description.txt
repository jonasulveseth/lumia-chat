H√§r kommer en uppdaterad och komplett **projektbeskrivning f√∂r Lumia**, nu med integrerad dokumentation f√∂r Brain och ut√∂kad specifikation f√∂r anv√§ndarhantering och lagring mot kundspecifika collections:

---

## **Projektnamn:** Lumia

**Typ:** Chatbaserad AI-tj√§nst
**Teknikstack:** Python, FastAPI, Ollama, LangChain, ChromaDB, RAG-modell, JWT-auth

---

### **Syfte och vision**

**Lumia** √§r en lokal, s√§ker och personlig AI-chattj√§nst som kombinerar styrkan i stora spr√•kmodeller (LLM) med ett kontinuerligt v√§xande personligt minne. Genom att koppla ihop en lokal spr√•kmodell (via Ollama) med en s√∂kmotor f√∂r kontext (Brain-tj√§nsten) som anv√§nder RAG och ChromaDB, bygger Lumia upp en anv√§ndarspecifik f√∂rst√•else som f√∂rb√§ttras √∂ver tid.

M√•let √§r att skapa en privat, responsiv assistent som f√∂rst√•r vem du √§r och anpassar sina svar ‚Äì utan att skicka n√•gon data till molnet.

---

### **Huvudfunktioner**

* **Chatbaserad anv√§ndarupplevelse** med direkt streaming av svar fr√•n lokal spr√•kmodell
* **Anv√§ndarspecifikt minne** som v√§xer √∂ver tid genom kontinuerlig ingestion av dialoghistorik
* **Parallell RAG-analys** via mikrotj√§nsten *Brain* f√∂r kontextuell f√∂rst√§rkning
* **S√§ker och lokal datalagring** via ChromaDB, med en collection per anv√§ndare
* **Registrering och inloggning** (med planerad JWT-autentisering)
* **Framtidss√§krad struktur** f√∂r att ut√∂ka med olika typer av kontextstrategier och specialiserade agents

---

### **System√∂versikt**

```plaintext
[Frontend (Lumia Chat UI)]
        ‚îÇ
        ‚ñº
[Python Backend (FastAPI)]
        ‚îú‚îÄ‚Üí [Ollama LLM] ‚îÄ (Streamar direkt svar fr√•n t.ex. Qwen 2.5)
        ‚îú‚îÄ‚Üí [Brain API (FastAPI)]
        ‚îÇ        ‚îú‚îÄ‚Üí ChromaDB (kundspecifika collections)
        ‚îÇ        ‚îî‚îÄ‚Üí Ollama (embeddings: nomic-embed-text)
        ‚îî‚îÄ‚Üí Anv√§ndardatabas (login, auth, collection-koppling)
```

---

### **Komponenter och funktionella delar**

#### üí¨ **1. LLM Chatmotor (via Ollama)**

* Lokal LLM-modell, t.ex. `qwen2.5:7b`
* Streamar svaret f√∂r snabb interaktion
* Anv√§nds alltid som prim√§r generator

#### üß† **2. Brain RAG Service**

* Frist√•ende FastAPI-mikrotj√§nst
* Tar emot fr√•gor och s√∂ker i kundspecifika ChromaDB-collections
* RAG-modell bygger svar via Ollama
* All embedding-generering sker lokalt (nomic-embed-text)
* Dashboard f√∂r dokumenthantering

#### üìö **3. Kontextinjektion**

* Svar fr√•n *Brain* kan komplettera eller ers√§tta svar fr√•n LLM
* Regler beh√∂vs f√∂r n√§r RAG-kontext ska anv√§ndas

  * Exempel: bara n√§r historisk tr√§ff √∂ver visst tr√∂skelv√§rde
  * Undvik √∂verdriven √•terkoppling till tidigare konversationer

#### üë§ **4. Anv√§ndarhantering**

* Registrering, inloggning och autentisering via JWT
* Varje anv√§ndare tilldelas en unik collection i Brain, ex: `lumia_100023`
* Alla chattar kan sparas automatiskt i denna collection
* M√∂jlighet att visa historik i framtiden via dashboard eller export

#### üîÑ **5. Sammanh√§ngande fl√∂de**

1. Anv√§ndaren skickar ett meddelande
2. Lumia skickar parallellt:

   * Till Ollama ‚Üí prim√§rt LLM-svar
   * Till Brain `/query` ‚Üí s√∂kning i ChromaDB
3. Svaren kombineras eller prioriteras baserat p√• kontextstrategi
4. Anv√§ndarens inl√§gg + svaret lagras i Brain `/ingest` under r√§tt collection
5. Chatten visas och streamas tillbaka till anv√§ndaren

---

### **Tekniska val**

| Komponent     | Verktyg / Modell            | Notering        |
| ------------- | --------------------------- | --------------- |
| LLM           | Ollama (`qwen2.5:7b`)       | Lokal           |
| Embeddings    | Ollama (`nomic-embed-text`) | Lokal           |
| Vektordatabas | ChromaDB                    | Per anv√§ndare   |
| Backend       | Python, FastAPI             | JWT, routing    |
| Kontextmotor  | Brain (FastAPI)             | RAG med LLM     |
| Orkestrering  | LangChain                   | Framtida kedjor |
| Anv√§ndardata  | PostgreSQL / SQLite         | Inloggning      |
| UI            | Web/terminal/chatbot        | Anpassningsbar  |

---

### **API-anrop mot Brain**

**/ingest**
Spara ny konversation:

```json
POST /ingest
{
  "customer_id": "lumia_100023",
  "content": "Anv√§ndarens fr√•ga + LLM-svar",
  "metadata": {
    "source": "chat",
    "timestamp": "2025-08-03T12:00:00Z"
  }
}
```

**/query**
S√∂k i tidigare kontext:

```json
POST /query
{
  "customer_id": "lumia_100023",
  "question": "Vad sa jag f√∂rra veckan om API-nycklar?",
  "n_results": 3
}
```

---

### **Roadmap**

| Version | Funktioner                                               |
| ------- | -------------------------------------------------------- |
| v0.1    | Chatgr√§nssnitt + streaming + enkel Brain-koppling        |
| v0.2    | Inloggning + JWT + sparande till ChromaDB                |
| v0.3    | Kontextstrategier (rules for retrieval)                  |
| v0.4    | Historikvy + anv√§ndarprofil                              |
| v0.5    | Multi-agent support + integrationer (e-post, Slack, etc) |

---

Vill du √§ven att jag s√§tter upp ett GitHub-README baserat p√• detta, inklusive exempelanrop och kodstruktur?
